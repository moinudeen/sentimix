{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.classical_models import SentimixModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      id sentiment                                               text  \\\n",
       "0  23081   neutral  RT @ RD _ BANA Kahan Ho ???? Zinda Samadhi Kab...   \n",
       "1  29854  negative  In pro-indian hazraat ka Bughazzay Pak fauj da...   \n",
       "2  35319   neutral  RT @ Sm4bjp @ sardesairajdeep Some media walas...   \n",
       "3   9572  positive  @ aapkadharam Hello sir ji üôèüôèüôèüôèüôè Sir ji mere d...   \n",
       "4  24598   neutral  @ OmarAyubKhan sir aaj subah sehri se light ka...   \n",
       "\n",
       "                                     language_labels  \\\n",
       "0  ['Eng', 'O', 'Hin', 'O', 'Hin', 'Hin', 'Hin', ...   \n",
       "1  ['Eng', 'Eng', 'Hin', 'Hin', 'Eng', 'Hin', 'En...   \n",
       "2  ['Eng', 'O', 'Eng', 'O', 'Hin', 'Hin', 'Eng', ...   \n",
       "3  ['O', 'Hin', 'Hin', 'Hin', 'Hin', 'O', 'Hin', ...   \n",
       "4  ['O', 'Hin', 'Hin', 'Hin', 'Hin', 'Hin', 'Hin'...   \n",
       "\n",
       "                                          clean_text  labels  \n",
       "0  rt mention rd bana kahan ho zinda samadhi kab ...       1  \n",
       "1  in proindian hazraat ka bughazzay pak fauj dai...       0  \n",
       "2  rt mention sm4bjp mention sardesairajdeep some...       1  \n",
       "3  mention aapkadharam hello sir ji sir ji mere d...       2  \n",
       "4  mention omarayubkhan sir aaj subah sehri se li...       1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sentiment</th>\n      <th>text</th>\n      <th>language_labels</th>\n      <th>clean_text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>23081</td>\n      <td>neutral</td>\n      <td>RT @ RD _ BANA Kahan Ho ???? Zinda Samadhi Kab...</td>\n      <td>['Eng', 'O', 'Hin', 'O', 'Hin', 'Hin', 'Hin', ...</td>\n      <td>rt mention rd bana kahan ho zinda samadhi kab ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>29854</td>\n      <td>negative</td>\n      <td>In pro-indian hazraat ka Bughazzay Pak fauj da...</td>\n      <td>['Eng', 'Eng', 'Hin', 'Hin', 'Eng', 'Hin', 'En...</td>\n      <td>in proindian hazraat ka bughazzay pak fauj dai...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>35319</td>\n      <td>neutral</td>\n      <td>RT @ Sm4bjp @ sardesairajdeep Some media walas...</td>\n      <td>['Eng', 'O', 'Eng', 'O', 'Hin', 'Hin', 'Eng', ...</td>\n      <td>rt mention sm4bjp mention sardesairajdeep some...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9572</td>\n      <td>positive</td>\n      <td>@ aapkadharam Hello sir ji üôèüôèüôèüôèüôè Sir ji mere d...</td>\n      <td>['O', 'Hin', 'Hin', 'Hin', 'Hin', 'O', 'Hin', ...</td>\n      <td>mention aapkadharam hello sir ji sir ji mere d...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24598</td>\n      <td>neutral</td>\n      <td>@ OmarAyubKhan sir aaj subah sehri se light ka...</td>\n      <td>['O', 'Hin', 'Hin', 'Hin', 'Hin', 'Hin', 'Hin'...</td>\n      <td>mention omarayubkhan sir aaj subah sehri se li...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/processed_train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 14374 entries, 0 to 14373\nData columns (total 6 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   id               14374 non-null  int64 \n 1   sentiment        14374 non-null  object\n 2   text             14374 non-null  object\n 3   language_labels  14374 non-null  object\n 4   clean_text       14374 non-null  object\n 5   labels           14374 non-null  int64 \ndtypes: int64(2), object(4)\nmemory usage: 673.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['clean_text'].values, data[\"labels\"].values.reshape(-1, 1), test_size=0.2, random_state=0)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 58 ¬µs, sys: 0 ns, total: 58 ¬µs\nWall time: 62.2 ¬µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_param_grid = {\n",
    "                'vectorizer__max_features': [50000, 100000],\n",
    "                'classifier__C': [0.1,1,5,10,100],\n",
    "                'classifier__penalty': ['l1', 'l2'],\n",
    "                    \n",
    "            }\n",
    "\n",
    "LR = LogisticRegression(C=4, max_iter=1000)\n",
    "tfidf = TfidfVectorizer(strip_accents=\"unicode\", max_features=100000, token_pattern='\\w+', ngram_range=(1, 2))\n",
    "\n",
    "lr = SentimixModel(vectorizer=tfidf, classifier=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.63      0.66      0.64       863\n           1       0.51      0.53      0.52      1002\n           2       0.72      0.66      0.69      1010\n\n    accuracy                           0.61      2875\n   macro avg       0.62      0.62      0.62      2875\nweighted avg       0.62      0.61      0.62      2875\n\nCPU times: user 8.83 s, sys: 1.09 s, total: 9.93 s\nWall time: 3min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "lr.train(X_train, y_train, random_search=True, param_grid=lr_param_grid)\n",
    "print(lr.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Params for Logistic Regression {'vectorizer__max_features': 50000, 'classifier__penalty': 'l2', 'classifier__C': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Params for Logistic Regression\", lr._model.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       4.574465      0.363149         0.000000        0.000000   \n",
       "1       3.563742      0.302562         0.000000        0.000000   \n",
       "2      12.071874      1.388220         0.880677        0.427337   \n",
       "3       3.454413      0.259546         0.000000        0.000000   \n",
       "4      21.939345      0.718899         0.634604        0.078964   \n",
       "5       3.611558      0.089805         0.000000        0.000000   \n",
       "6      53.279123      1.412098         0.628104        0.097140   \n",
       "7      21.704108      1.671295         0.632774        0.072101   \n",
       "8      33.149220      2.695113         0.766105        0.331796   \n",
       "9       4.218071      0.578347         0.000000        0.000000   \n",
       "\n",
       "  param_vectorizer__max_features param_classifier__penalty  \\\n",
       "0                          50000                        l1   \n",
       "1                         100000                        l1   \n",
       "2                          50000                        l2   \n",
       "3                          50000                        l1   \n",
       "4                          50000                        l2   \n",
       "5                         100000                        l1   \n",
       "6                         100000                        l2   \n",
       "7                         100000                        l2   \n",
       "8                         100000                        l2   \n",
       "9                          50000                        l1   \n",
       "\n",
       "  param_classifier__C                                             params  \\\n",
       "0                   5  {'vectorizer__max_features': 50000, 'classifie...   \n",
       "1                   5  {'vectorizer__max_features': 100000, 'classifi...   \n",
       "2                   1  {'vectorizer__max_features': 50000, 'classifie...   \n",
       "3                   1  {'vectorizer__max_features': 50000, 'classifie...   \n",
       "4                  10  {'vectorizer__max_features': 50000, 'classifie...   \n",
       "5                 100  {'vectorizer__max_features': 100000, 'classifi...   \n",
       "6                 100  {'vectorizer__max_features': 100000, 'classifi...   \n",
       "7                   1  {'vectorizer__max_features': 100000, 'classifi...   \n",
       "8                  10  {'vectorizer__max_features': 100000, 'classifi...   \n",
       "9                 100  {'vectorizer__max_features': 50000, 'classifie...   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0                NaN                NaN                NaN                NaN   \n",
       "1                NaN                NaN                NaN                NaN   \n",
       "2           0.603913           0.598261           0.615217           0.600435   \n",
       "3                NaN                NaN                NaN                NaN   \n",
       "4           0.600870           0.587391           0.600000           0.580000   \n",
       "5                NaN                NaN                NaN                NaN   \n",
       "6           0.601304           0.586522           0.598261           0.581304   \n",
       "7           0.599565           0.595217           0.619565           0.599130   \n",
       "8           0.603913           0.596522           0.606522           0.587391   \n",
       "9                NaN                NaN                NaN                NaN   \n",
       "\n",
       "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0                NaN              NaN             NaN                6  \n",
       "1                NaN              NaN             NaN                7  \n",
       "2           0.614180         0.606401        0.007018                1  \n",
       "3                NaN              NaN             NaN                8  \n",
       "4           0.601131         0.593878        0.008645                5  \n",
       "5                NaN              NaN             NaN                9  \n",
       "6           0.608525         0.595183        0.009926                4  \n",
       "7           0.613745         0.605445        0.009459                2  \n",
       "8           0.609395         0.600749        0.007930                3  \n",
       "9                NaN              NaN             NaN               10  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_vectorizer__max_features</th>\n      <th>param_classifier__penalty</th>\n      <th>param_classifier__C</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.574465</td>\n      <td>0.363149</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>50000</td>\n      <td>l1</td>\n      <td>5</td>\n      <td>{'vectorizer__max_features': 50000, 'classifie...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3.563742</td>\n      <td>0.302562</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>100000</td>\n      <td>l1</td>\n      <td>5</td>\n      <td>{'vectorizer__max_features': 100000, 'classifi...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12.071874</td>\n      <td>1.388220</td>\n      <td>0.880677</td>\n      <td>0.427337</td>\n      <td>50000</td>\n      <td>l2</td>\n      <td>1</td>\n      <td>{'vectorizer__max_features': 50000, 'classifie...</td>\n      <td>0.603913</td>\n      <td>0.598261</td>\n      <td>0.615217</td>\n      <td>0.600435</td>\n      <td>0.614180</td>\n      <td>0.606401</td>\n      <td>0.007018</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.454413</td>\n      <td>0.259546</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>50000</td>\n      <td>l1</td>\n      <td>1</td>\n      <td>{'vectorizer__max_features': 50000, 'classifie...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>21.939345</td>\n      <td>0.718899</td>\n      <td>0.634604</td>\n      <td>0.078964</td>\n      <td>50000</td>\n      <td>l2</td>\n      <td>10</td>\n      <td>{'vectorizer__max_features': 50000, 'classifie...</td>\n      <td>0.600870</td>\n      <td>0.587391</td>\n      <td>0.600000</td>\n      <td>0.580000</td>\n      <td>0.601131</td>\n      <td>0.593878</td>\n      <td>0.008645</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3.611558</td>\n      <td>0.089805</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>100000</td>\n      <td>l1</td>\n      <td>100</td>\n      <td>{'vectorizer__max_features': 100000, 'classifi...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>53.279123</td>\n      <td>1.412098</td>\n      <td>0.628104</td>\n      <td>0.097140</td>\n      <td>100000</td>\n      <td>l2</td>\n      <td>100</td>\n      <td>{'vectorizer__max_features': 100000, 'classifi...</td>\n      <td>0.601304</td>\n      <td>0.586522</td>\n      <td>0.598261</td>\n      <td>0.581304</td>\n      <td>0.608525</td>\n      <td>0.595183</td>\n      <td>0.009926</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>21.704108</td>\n      <td>1.671295</td>\n      <td>0.632774</td>\n      <td>0.072101</td>\n      <td>100000</td>\n      <td>l2</td>\n      <td>1</td>\n      <td>{'vectorizer__max_features': 100000, 'classifi...</td>\n      <td>0.599565</td>\n      <td>0.595217</td>\n      <td>0.619565</td>\n      <td>0.599130</td>\n      <td>0.613745</td>\n      <td>0.605445</td>\n      <td>0.009459</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>33.149220</td>\n      <td>2.695113</td>\n      <td>0.766105</td>\n      <td>0.331796</td>\n      <td>100000</td>\n      <td>l2</td>\n      <td>10</td>\n      <td>{'vectorizer__max_features': 100000, 'classifi...</td>\n      <td>0.603913</td>\n      <td>0.596522</td>\n      <td>0.606522</td>\n      <td>0.587391</td>\n      <td>0.609395</td>\n      <td>0.600749</td>\n      <td>0.007930</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>4.218071</td>\n      <td>0.578347</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>50000</td>\n      <td>l1</td>\n      <td>100</td>\n      <td>{'vectorizer__max_features': 50000, 'classifie...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "pd.DataFrame(lr._model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.save(\"../weights/sentimix_logistic_regression.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 1.26 s, sys: 342 ms, total: 1.6 s\nWall time: 2.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "lr2 = SentimixModel(model_path='../weights/sentimix_logistic_regression.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.63      0.66      0.64       863\n           1       0.51      0.53      0.52      1002\n           2       0.72      0.66      0.69      1010\n\n    accuracy                           0.61      2875\n   macro avg       0.62      0.62      0.62      2875\nweighted avg       0.62      0.61      0.62      2875\n\n"
     ]
    }
   ],
   "source": [
    "print(lr2.evaluate(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3.16 ms, sys: 1.97 ms, total: 5.13 ms\nWall time: 6.52 ms\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "%%time\n",
    "lr.predict(np.array([\"kaise hain yaar\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}