{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_sentimixTransformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi2Fx2s9302l",
        "outputId": "b385061b-3886-4319-ff93-686f91e2541d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jan 14 18:06:17 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    28W /  70W |   7677MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import XLMRobertaTokenizer, DistilBertForSequenceClassification, \\\n",
        "                         DistilBertTokenizerFast, XLMRobertaForSequenceClassification"
      ]
    },
    {
      "source": [
        "## XLM Roberta Model"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "VRhKiTTq34Hh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvwe8_SO5EPy"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=10,              # total number of training epochs\n",
        "    per_device_train_batch_size=32,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=1000,\n",
        ")\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_bqj7d65S1-",
        "outputId": "45320938-e825-42e5-ad87-9972f228d6f5"
      },
      "source": [
        "xlmroberta = SentimixTransformer(\n",
        "                                 XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=3),\n",
        "                                 XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "                                 )"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRsAdpux5Z3w"
      },
      "source": [
        "train_data, test_data, labels = xlmroberta.preprocess_data(\"../data/processed_train.csv\", \"clean_text\", \"labels\")"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "1QERnkc-50mk",
        "outputId": "98555464-c426-43d2-8db3-087a4ee74846"
      },
      "source": [
        "xlmroberta.train(training_args, train_data)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='405' max='405' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [405/405 03:47, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "2oRPFgU059Ys",
        "outputId": "ab524213-678c-4b06-9f57-37112a4a4f30"
      },
      "source": [
        "xlmroberta.evaluate(test_data)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [23/23 00:05]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 1.0,\n",
              " 'eval_accuracy': 0.5880306193458594,\n",
              " 'eval_f1': 0.5693027149916818,\n",
              " 'eval_loss': 0.868428647518158,\n",
              " 'eval_precision': 0.5823817952393507,\n",
              " 'eval_recall': 0.5880306193458594,\n",
              " 'eval_runtime': 6.1171,\n",
              " 'eval_samples_per_second': 234.914}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SKCt2ff6g5l"
      },
      "source": [
        "xlmroberta.save_model(\"../weights/xlm_roberta/\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZecYUGmbIFtv"
      },
      "source": [
        "xlmroberta.build_pipeline()"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "YeHq8eElIVlC",
        "outputId": "d7e808e1-f032-4ec9-fa31-a906a3f53e3d"
      },
      "source": [
        "\n",
        "testset = pd.read_csv('../data/processed_val.csv')\n",
        "# testset.shape\n",
        "\n",
        "testset['xlmroberta_output'] = testset['clean_text'].apply(lambda z: xlmroberta.inference_pipeline(z)[0])\n",
        "testset['xlmroberta_prediction'] = testset['xlmroberta_output'].apply(lambda z: labels[xlmroberta.model.config.label2id[z['label']]])\n",
        "testset['xlmroberta_confidence'] = testset['xlmroberta_output'].apply(lambda z: z['score'])\n",
        "testset.to_csv(\"../data/outputs/testset_xlmroberta_output.csv\", index=False)\n",
        "\n",
        "testset.head()\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "      <th>language_labels</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>labels</th>\n",
              "      <th>xlmroberta_output</th>\n",
              "      <th>xlmroberta_prediction</th>\n",
              "      <th>xlmroberta_confidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>182</td>\n",
              "      <td>negative</td>\n",
              "      <td>@ tamashbeen _ Well chara Chor ke chele this n...</td>\n",
              "      <td>['O', 'Eng', 'O', 'Eng', 'Eng', 'Hin', 'Hin', ...</td>\n",
              "      <td>mention tamashbeen well chara chor ke chele th...</td>\n",
              "      <td>0</td>\n",
              "      <td>{'label': 'LABEL_0', 'score': 0.45618289709091...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.456183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>701</td>\n",
              "      <td>positive</td>\n",
              "      <td>@ suhailswarraich Journals k bare me . Bt ki i...</td>\n",
              "      <td>['O', 'Eng', 'Eng', 'Eng', 'Hin', 'Hin', 'O', ...</td>\n",
              "      <td>mention suhailswarraich journals k bare me bt ...</td>\n",
              "      <td>2</td>\n",
              "      <td>{'label': 'LABEL_0', 'score': 0.5885549783706665}</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.588555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1093</td>\n",
              "      <td>negative</td>\n",
              "      <td>@ SKFforlife Haha Overrated kisko bolte hain c...</td>\n",
              "      <td>['O', 'Hin', 'Hin', 'Hin', 'Hin', 'Hin', 'Hin'...</td>\n",
              "      <td>mention skfforlife haha overrated kisko bolte ...</td>\n",
              "      <td>0</td>\n",
              "      <td>{'label': 'LABEL_0', 'score': 0.5542877912521362}</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.554288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1106</td>\n",
              "      <td>positive</td>\n",
              "      <td>I wanted to see you in Loksabha @ kanhaiyakuma...</td>\n",
              "      <td>['Eng', 'Eng', 'Eng', 'Eng', 'Eng', 'Eng', 'Hi...</td>\n",
              "      <td>i wanted to see you in loksabha mention kanhai...</td>\n",
              "      <td>2</td>\n",
              "      <td>{'label': 'LABEL_2', 'score': 0.7313329577445984}</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.731333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1156</td>\n",
              "      <td>negative</td>\n",
              "      <td>@ abpnewshindi ABP news 5 saal se mon nahi bha...</td>\n",
              "      <td>['O', 'Hin', 'Eng', 'Eng', 'O', 'Hin', 'Hin', ...</td>\n",
              "      <td>mention abpnewshindi abp news 5 saal se mon na...</td>\n",
              "      <td>0</td>\n",
              "      <td>{'label': 'LABEL_0', 'score': 0.6337724924087524}</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.633772</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id sentiment  ... xlmroberta_prediction xlmroberta_confidence\n",
              "0   182  negative  ...              negative              0.456183\n",
              "1   701  positive  ...              negative              0.588555\n",
              "2  1093  negative  ...              negative              0.554288\n",
              "3  1106  positive  ...              positive              0.731333\n",
              "4  1156  negative  ...              negative              0.633772\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvR9d_MwIYiZ",
        "outputId": "5e27560a-891e-4811-fdf2-48bf3bcc0f46"
      },
      "source": [
        "xlmroberta.inference_pipeline(\"hello \")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'LABEL_1', 'score': 0.5788123607635498}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E71P7k9WG3Z"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Distilbert Model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_1',          # output directory\n",
        "    num_train_epochs=10,              # total number of training epochs\n",
        "    per_device_train_batch_size=32,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs_1',            # directory for storing logs\n",
        "    logging_steps=1000,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "distilbert = SentimixTransformer(\n",
        "                                 DistilBertForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=3),\n",
        "                                 DistilBertTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data, test_data, labels = distilbert.preprocess_data(\"../data/processed_train.csv\", \"clean_text\", \"labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "distilbert.train(training_args, train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "distilbert.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "distilbert.save_model(\"../weights/distilbert/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "distilbert.build_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "testset = pd.read_csv('../data/processed_val.csv')\n",
        "# testset.shape\n",
        "\n",
        "testset['distilbert_output'] = testset['clean_text'].apply(lambda z: distilbert.inference_pipeline(z)[0])\n",
        "testset['distilbert_prediction'] = testset['distilbert_output'].apply(lambda z: labels[distilbert.model.config.label2id[z['label']]])\n",
        "testset['distilbert_confidence'] = testset['distilbert_output'].apply(lambda z: z['score'])\n",
        "testset.to_csv(\"../data/outputs/testset_distilbert_output.csv\", index=False)\n",
        "\n",
        "testset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "distilbert.inference_pipeline(\"hello \")"
      ]
    }
  ]
}